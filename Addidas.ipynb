{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d33b3a19-83c6-431d-945a-bb8fceee85f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"no style with name 'Italic'\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 126\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to scrape the website. Exiting the process.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 126\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[1], line 110\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    107\u001b[0m cleaned_text \u001b[38;5;241m=\u001b[39m clean_text(scraped_data)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# Step 4: Save the cleaned content to a new DOCX file with URL\u001b[39;00m\n\u001b[1;32m--> 110\u001b[0m save_to_docx(cleaned_text, url, output_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScraped_Content.docx\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m# Step 5: Optionally, save the cleaned content to a PDF file with URL\u001b[39;00m\n\u001b[0;32m    113\u001b[0m save_to_pdf(cleaned_text, url, output_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScraped_Content.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 46\u001b[0m, in \u001b[0;36msave_to_docx\u001b[1;34m(cleaned_text, url, output_path)\u001b[0m\n\u001b[0;32m     44\u001b[0m doc \u001b[38;5;241m=\u001b[39m Document()\n\u001b[0;32m     45\u001b[0m doc\u001b[38;5;241m.\u001b[39madd_heading(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScraped Content\u001b[39m\u001b[38;5;124m'\u001b[39m, level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 46\u001b[0m doc\u001b[38;5;241m.\u001b[39madd_paragraph(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mURL: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, style\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mItalic\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Add URL at the top\u001b[39;00m\n\u001b[0;32m     48\u001b[0m paragraphs \u001b[38;5;241m=\u001b[39m cleaned_text\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m para \u001b[38;5;129;01min\u001b[39;00m paragraphs:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\docx\\document.py:69\u001b[0m, in \u001b[0;36mDocument.add_paragraph\u001b[1;34m(self, text, style)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_paragraph\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, style: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m ParagraphStyle \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Paragraph:\n\u001b[0;32m     60\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return paragraph newly added to the end of the document.\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \n\u001b[0;32m     62\u001b[0m \u001b[38;5;124;03m    The paragraph is populated with `text` and having paragraph style `style`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03m    break.\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_body\u001b[38;5;241m.\u001b[39madd_paragraph(text, style)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\docx\\blkcntnr.py:57\u001b[0m, in \u001b[0;36mBlockItemContainer.add_paragraph\u001b[1;34m(self, text, style)\u001b[0m\n\u001b[0;32m     55\u001b[0m     paragraph\u001b[38;5;241m.\u001b[39madd_run(text)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m style \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 57\u001b[0m     paragraph\u001b[38;5;241m.\u001b[39mstyle \u001b[38;5;241m=\u001b[39m style\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m paragraph\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\docx\\text\\paragraph.py:146\u001b[0m, in \u001b[0;36mParagraph.style\u001b[1;34m(self, style_or_name)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;129m@style\u001b[39m\u001b[38;5;241m.\u001b[39msetter\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstyle\u001b[39m(\u001b[38;5;28mself\u001b[39m, style_or_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m ParagraphStyle \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 146\u001b[0m     style_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpart\u001b[38;5;241m.\u001b[39mget_style_id(style_or_name, WD_STYLE_TYPE\u001b[38;5;241m.\u001b[39mPARAGRAPH)\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_p\u001b[38;5;241m.\u001b[39mstyle \u001b[38;5;241m=\u001b[39m style_id\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\docx\\parts\\document.py:80\u001b[0m, in \u001b[0;36mDocumentPart.get_style_id\u001b[1;34m(self, style_or_name, style_type)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_style_id\u001b[39m(\u001b[38;5;28mself\u001b[39m, style_or_name, style_type):\n\u001b[0;32m     73\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the style_id (|str|) of the style of `style_type` matching\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;124;03m    `style_or_name`.\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;124;03m    wrong type or names a style not present in the document.\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstyles\u001b[38;5;241m.\u001b[39mget_style_id(style_or_name, style_type)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\docx\\styles\\styles.py:101\u001b[0m, in \u001b[0;36mStyles.get_style_id\u001b[1;34m(self, style_or_name, style_type)\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_style_id_from_style(style_or_name, style_type)\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_style_id_from_name(style_or_name, style_type)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\docx\\styles\\styles.py:130\u001b[0m, in \u001b[0;36mStyles._get_style_id_from_name\u001b[1;34m(self, style_name, style_type)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_style_id_from_name\u001b[39m(\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28mself\u001b[39m, style_name: \u001b[38;5;28mstr\u001b[39m, style_type: WD_STYLE_TYPE\n\u001b[0;32m    123\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the id of the style of `style_type` corresponding to `style_name`.\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \n\u001b[0;32m    126\u001b[0m \u001b[38;5;124;03m    Returns |None| if that style is the default style for `style_type`. Raises\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m    |ValueError| if the named style is not found in the document or does not match\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    `style_type`.\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_style_id_from_style(\u001b[38;5;28mself\u001b[39m[style_name], style_type)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\docx\\styles\\styles.py:50\u001b[0m, in \u001b[0;36mStyles.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     47\u001b[0m     warn(msg, \u001b[38;5;167;01mUserWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m StyleFactory(style_elm)\n\u001b[1;32m---> 50\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno style with name \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m key)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"no style with name 'Italic'\""
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from docx import Document\n",
    "from fpdf import FPDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# Ensure required NLTK resources are available\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Function to scrape data from a webpage\n",
    "def scrape_website(url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        return soup.get_text(separator='\\n')\n",
    "    else:\n",
    "        print(f\"Failed to scrape the website. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Function to clean scraped text\n",
    "def clean_text(text):\n",
    "    # Remove unnecessary information using regular expressions\n",
    "    cleaned_text = re.sub(r'(subscribe|find your story|download media|follow us|contact us|privacy notice|cookie policy|social media|legal|corporate news|cookies|site uses cookies|download all|media cart|press release|related links)', '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Remove extra whitespace and fix inconsistent capitalization\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    \n",
    "    # Fix sentence capitalization\n",
    "    cleaned_text = '. '.join(sentence.capitalize() for sentence in cleaned_text.split('. '))\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Function to save cleaned text to DOCX with URL\n",
    "def save_to_docx(cleaned_text, url, output_path='Scraped_Content.docx'):\n",
    "    doc = Document()\n",
    "    doc.add_heading('Scraped Content', level=1)\n",
    "    doc.add_paragraph(f\"URL: {url}\", style='Italic')  # Add URL at the top\n",
    "\n",
    "    paragraphs = cleaned_text.split(\"\\n\\n\")\n",
    "    for para in paragraphs:\n",
    "        doc.add_paragraph(para)\n",
    "    \n",
    "    doc.save(output_path)\n",
    "    print(f\"Cleaned content saved to {output_path}\")\n",
    "\n",
    "# Function to save cleaned text to PDF with URL\n",
    "def save_to_pdf(cleaned_text, url, output_path='Scraped_Content.pdf'):\n",
    "    pdf = FPDF()\n",
    "    pdf.set_auto_page_break(auto=True, margin=15)\n",
    "    pdf.add_page()\n",
    "    pdf.set_font(\"Arial\", size=12)\n",
    "\n",
    "    # Add URL at the top of the PDF\n",
    "    pdf.multi_cell(0, 10, f\"URL: {url}\\n\")\n",
    "\n",
    "    # Add cleaned text\n",
    "    pdf.multi_cell(0, 10, cleaned_text)\n",
    "    \n",
    "    pdf.output(output_path)\n",
    "    print(f\"Cleaned content saved to {output_path}\")\n",
    "\n",
    "# Function to preprocess text for summarization\n",
    "def preprocess_text(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    processed_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence.lower())\n",
    "        words_filtered = [word for word in words if word.isalnum() and word not in stop_words]\n",
    "        processed_sentences.append(\" \".join(words_filtered))\n",
    "    \n",
    "    return sentences, processed_sentences\n",
    "\n",
    "# Function to perform extractive summarization using TF-IDF\n",
    "def extractive_summary(text, num_sentences=5):\n",
    "    original_sentences, processed_sentences = preprocess_text(text)\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(processed_sentences)\n",
    "\n",
    "    sentence_scores = tfidf_matrix.sum(axis=1)\n",
    "    ranked_sentences = [(score, sent) for score, sent in zip(sentence_scores, original_sentences)]\n",
    "    ranked_sentences.sort(reverse=True, key=lambda x: x[0])\n",
    "\n",
    "    summary = \" \".join([sent for _, sent in ranked_sentences[:num_sentences]])\n",
    "    return summary\n",
    "\n",
    "# Main function to scrape, clean, save, and summarize data\n",
    "def main():\n",
    "    # URL to scrape\n",
    "    url = \"https://news.adidas.com/training/adidas-brings-universal-design-principles-to-kit-for-paris-2024--to-optimise-fit-and-performance-for/s/675a0e1a-ddd7-428f-b15d-1713d59bb352\"\n",
    "\n",
    "    # Step 2: Scrape the website content\n",
    "    scraped_data = scrape_website(url)\n",
    "    \n",
    "    if scraped_data:\n",
    "        # Step 3: Clean the scraped text\n",
    "        cleaned_text = clean_text(scraped_data)\n",
    "        \n",
    "        # Step 4: Save the cleaned content to a new DOCX file with URL\n",
    "        save_to_docx(cleaned_text, url, output_path='Scraped_Content.docx')\n",
    "        \n",
    "        # Step 5: Optionally, save the cleaned content to a PDF file with URL\n",
    "        save_to_pdf(cleaned_text, url, output_path='Scraped_Content.pdf')\n",
    "\n",
    "        # Step 6: Ask the user if they want to generate a summary\n",
    "        summary_choice = input(\"Would you like to generate a summary? (y/n): \")\n",
    "        if summary_choice.lower() == 'y':\n",
    "            summary = extractive_summary(cleaned_text, num_sentences=5)\n",
    "            print(\"\\nSummary:\\n\", summary)\n",
    "        else:\n",
    "            print(\"Summary generation skipped.\")\n",
    "    else:\n",
    "        print(\"Failed to scrape the website. Exiting the process.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f054ebc-58e7-487d-b57f-3fcefda1441c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned content saved to Scraped_Content.docx\n"
     ]
    },
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'latin-1' codec can't encode character '\\u2019' in position 1171: ordinal not in range(256)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 130\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to scrape the website. Exiting the process.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 130\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[3], line 117\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    114\u001b[0m save_to_docx(cleaned_text, url, output_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScraped_Content.docx\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# Step 5: Optionally, save the cleaned content to a PDF file with URL\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m save_to_pdf(cleaned_text, url, output_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScraped_Content.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m# Step 6: Ask the user if they want to generate a summary\u001b[39;00m\n\u001b[0;32m    120\u001b[0m summary_choice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWould you like to generate a summary? (y/n): \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 72\u001b[0m, in \u001b[0;36msave_to_pdf\u001b[1;34m(cleaned_text, url, output_path)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Add cleaned text\u001b[39;00m\n\u001b[0;32m     70\u001b[0m pdf\u001b[38;5;241m.\u001b[39mmulti_cell(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m10\u001b[39m, cleaned_text)\n\u001b[1;32m---> 72\u001b[0m pdf\u001b[38;5;241m.\u001b[39moutput(output_path)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCleaned content saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\fpdf\\fpdf.py:1065\u001b[0m, in \u001b[0;36mFPDF.output\u001b[1;34m(self, name, dest)\u001b[0m\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;66;03m#Finish document if necessary\u001b[39;00m\n\u001b[0;32m   1064\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m-> 1065\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m   1066\u001b[0m dest\u001b[38;5;241m=\u001b[39mdest\u001b[38;5;241m.\u001b[39mupper()\n\u001b[0;32m   1067\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(dest\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\fpdf\\fpdf.py:246\u001b[0m, in \u001b[0;36mFPDF.close\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_endpage()\n\u001b[0;32m    245\u001b[0m \u001b[38;5;66;03m#close document\u001b[39;00m\n\u001b[1;32m--> 246\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enddoc()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\fpdf\\fpdf.py:1636\u001b[0m, in \u001b[0;36mFPDF._enddoc\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1634\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_enddoc\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1635\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_putheader()\n\u001b[1;32m-> 1636\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_putpages()\n\u001b[0;32m   1637\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_putresources()\n\u001b[0;32m   1638\u001b[0m     \u001b[38;5;66;03m#Info\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\fpdf\\fpdf.py:1170\u001b[0m, in \u001b[0;36mFPDF._putpages\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1167\u001b[0m \u001b[38;5;66;03m#Page content\u001b[39;00m\n\u001b[0;32m   1168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompress:\n\u001b[0;32m   1169\u001b[0m     \u001b[38;5;66;03m# manage binary data as latin1 until PEP461 or similar is implemented\u001b[39;00m\n\u001b[1;32m-> 1170\u001b[0m     p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpages[n]\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m PY3K \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpages[n] \n\u001b[0;32m   1171\u001b[0m     p \u001b[38;5;241m=\u001b[39m zlib\u001b[38;5;241m.\u001b[39mcompress(p)\n\u001b[0;32m   1172\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'latin-1' codec can't encode character '\\u2019' in position 1171: ordinal not in range(256)"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from docx import Document\n",
    "from fpdf import FPDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# Ensure required NLTK resources are available\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Function to scrape data from a webpage\n",
    "def scrape_website(url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        return soup.get_text(separator='\\n')\n",
    "    else:\n",
    "        print(f\"Failed to scrape the website. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Function to clean scraped text\n",
    "def clean_text(text):\n",
    "    # Remove unnecessary information using regular expressions\n",
    "    cleaned_text = re.sub(r'(subscribe|find your story|download media|follow us|contact us|privacy notice|cookie policy|social media|legal|corporate news|cookies|site uses cookies|download all|media cart|press release|related links)', '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Remove extra whitespace and fix inconsistent capitalization\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    \n",
    "    # Fix sentence capitalization\n",
    "    cleaned_text = '. '.join(sentence.capitalize() for sentence in cleaned_text.split('. '))\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Function to save cleaned text to DOCX with URL\n",
    "def save_to_docx(cleaned_text, url, output_path='Scraped_Content.docx'):\n",
    "    doc = Document()\n",
    "    doc.add_heading('Scraped Content', level=1)\n",
    "\n",
    "    # Add the URL in italic style\n",
    "    para = doc.add_paragraph()\n",
    "    run = para.add_run(f\"URL: {url}\")\n",
    "    run.italic = True\n",
    "\n",
    "    paragraphs = cleaned_text.split(\"\\n\\n\")\n",
    "    for para in paragraphs:\n",
    "        doc.add_paragraph(para)\n",
    "    \n",
    "    doc.save(output_path)\n",
    "    print(f\"Cleaned content saved to {output_path}\")\n",
    "\n",
    "# Function to save cleaned text to PDF with URL\n",
    "def save_to_pdf(cleaned_text, url, output_path='Scraped_Content.pdf'):\n",
    "    pdf = FPDF()\n",
    "    pdf.set_auto_page_break(auto=True, margin=15)\n",
    "    pdf.add_page()\n",
    "    pdf.set_font(\"Arial\", size=12)\n",
    "\n",
    "    # Add URL at the top of the PDF\n",
    "    pdf.multi_cell(0, 10, f\"URL: {url}\\n\")\n",
    "\n",
    "    # Add cleaned text\n",
    "    pdf.multi_cell(0, 10, cleaned_text)\n",
    "    \n",
    "    pdf.output(output_path)\n",
    "    print(f\"Cleaned content saved to {output_path}\")\n",
    "\n",
    "# Function to preprocess text for summarization\n",
    "def preprocess_text(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    processed_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence.lower())\n",
    "        words_filtered = [word for word in words if word.isalnum() and word not in stop_words]\n",
    "        processed_sentences.append(\" \".join(words_filtered))\n",
    "    \n",
    "    return sentences, processed_sentences\n",
    "\n",
    "# Function to perform extractive summarization using TF-IDF\n",
    "def extractive_summary(text, num_sentences=5):\n",
    "    original_sentences, processed_sentences = preprocess_text(text)\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(processed_sentences)\n",
    "\n",
    "    sentence_scores = tfidf_matrix.sum(axis=1)\n",
    "    ranked_sentences = [(score, sent) for score, sent in zip(sentence_scores, original_sentences)]\n",
    "    ranked_sentences.sort(reverse=True, key=lambda x: x[0])\n",
    "\n",
    "    summary = \" \".join([sent for _, sent in ranked_sentences[:num_sentences]])\n",
    "    return summary\n",
    "\n",
    "# Main function to scrape, clean, save, and summarize data\n",
    "def main():\n",
    "    # URL to scrape\n",
    "    url = \"https://news.adidas.com/training/adidas-brings-universal-design-principles-to-kit-for-paris-2024--to-optimise-fit-and-performance-for/s/675a0e1a-ddd7-428f-b15d-1713d59bb352\"\n",
    "\n",
    "    # Step 2: Scrape the website content\n",
    "    scraped_data = scrape_website(url)\n",
    "    \n",
    "    if scraped_data:\n",
    "        # Step 3: Clean the scraped text\n",
    "        cleaned_text = clean_text(scraped_data)\n",
    "        \n",
    "        # Step 4: Save the cleaned content to a new DOCX file with URL\n",
    "        save_to_docx(cleaned_text, url, output_path='Scraped_Content.docx')\n",
    "        \n",
    "        # Step 5: Optionally, save the cleaned content to a PDF file with URL\n",
    "        save_to_pdf(cleaned_text, url, output_path='Scraped_Content.pdf')\n",
    "\n",
    "        # Step 6: Ask the user if they want to generate a summary\n",
    "        summary_choice = input(\"Would you like to generate a summary? (y/n): \")\n",
    "        if summary_choice.lower() == 'y':\n",
    "            summary = extractive_summary(cleaned_text, num_sentences=5)\n",
    "            print(\"\\nSummary:\\n\", summary)\n",
    "        else:\n",
    "            print(\"Summary generation skipped.\")\n",
    "    else:\n",
    "        print(\"Failed to scrape the website. Exiting the process.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cd0c0c9-cc41-4bac-bbeb-fa1e73b04689",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned content saved to Scraped_Content.docx\n",
      "Cleaned content saved to Scraped_Content.pdf\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Would you like to generate a summary? (y/n):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary:\n",
      " African republic chad chile china colombia comoros congo cook island costa rica croatia cuba cyprus czech republic denmark djibouti dominica dominican republic ecuador egypt el salvador equatorial guinea eritrea estonia ethiopia fiji finland france french polynesia gabon gambia georgia germany ghana gibraltar greece greenland grenada guadeloupe guam guatemala guinea guinea-bissau guyana haiti honduras hong kong hungary iceland india indonesia iran iraq ireland israel italy ivory coast jamaica japan jordan kazakhstan kenya korea, republic kosovo kuwait kyrgyzstan laos latvia lebanon lesotho liberia libya liechtenstein lithuania lithunia luxembourg macau macedonia madagascar madeira islands malawi malaysia maldives mali malta marshall islands martinique mauritania mauritius mexico micronesia moldova monaco mongolia montenegro montserrat morocco mozambique myanmar (burma) namibia nepal netherlands netherlands antilles new caledonia new zealand nicaragua niger nigeria north korea norway oman others pakistan palau palestine panama papua new guinea paraguay peru philippines poland portugal puerto rico qatar republic of ireland republic of kiribati reunion islands romania russia rwanda s.tome and principe saba saint vincent and the grenadines san marino saudi arabia scotland senegal serbia seychelles sierra leone singapore slovak republic slovakia slovenia solomon islands somalia south africa south korea spain sri lanka st kitts&nevi st kitts&nevis st. Lucia st. Maarten sudan suriname swaziland sweden switzerland syria taiwan tajikistan tanzania thailand timor togo tonga trinidad & tobago tunisia turkey turkmenistan turks & caicos tuvalu uganda ukraine united arab emirates united kingdom united states uruguay uzbekistan vanuatu vatican city venezuela vietnam wales western samoa yemen zaire zambia zimbabwe message * first name * last name * email address * who are you? Adidas brings universal design principles to kit for paris 2024, to optimise fit and performance for all news sports performance american football baseball basketball football golf ice hockey outdoor rugby running skateboarding swimming tennis training brands originals sportswear partnerships product news footwear apparel teams purpose people planet innovations futurecraft 4d boost primeknit back home training adidas brings universal design principles to kit for paris 2024, to optimise fit and performance for all 18-apr-2024 paris created in close collaboration with athletes with and without a disability, most of adidas’ on and off field pieces feature key design edits to enable all to participate in comfort adidas launches adaptive training wear pieces for the first time and introduces the brand’s first pattern and finishes crafted specifically for athletes who play sports while seated or in a wheelchair product featuring universal design – including the adaptive training outfit – are available from 19th april, on www.adidas.com and at key retailers today, at the reveal of the adidas kits for the paris 2024 olympic and paralympic games, the sports brand has announced that 81% of pieces of apparel worn on and off the field of play have been created using design principles that ensure they work for athletes with and without a disability. These considerations – applied in varying ways across the collections – include smart placement and/or inclusion of seams, avoiding light colours on the cuffs of performance wear to optimise aesthetics following dirt transfer from wheels, using soft threading where possible, moving team iconography from covered areas – such as the lower back – onto those that are visible while seated, changing trims to be adaptive-first to work around limited movement, removal of any complicated fastenings including small buttons or zips, and offering zipped ankle cuffs for ease of dressing for those with limb difference, and opening a wider range of product lengths for athletes of different heights. Accept & close category of interest sports adidas by adidas by stella mccartney adidas pharrell williams athletics originals partnerships sportswear y-3 yeezy other country of origin afghanistan albania algeria american samoa andorra angola anguilla antigua/barbuda argentina armenia aruba australia austria azerbaijan azores bahamas bahrain bangladesh barbados belarus belgium belize benin bermuda bhutan bolivia bosnia-herzegovina botswana brazil british virgin islands brunei bulgaria burkino faso burundi cambodia cameroon canada cape verde cayman islands cen. For guidance on relevant language to tell these stories – please consult the un’s disability inclusive communications guidelines [1] world health organisation [2] due to additional partner signings as of july 2024, 81% of base styles have been created using universal design principles add all to cart +16 en adidas brings universal design principles to kit for paris 2024, to optimise fit and performance for all adidas adidas group related document disability-inclusive communications guidelines download file add file to view 2 / 19 files add all files to cart view back to top download file remove file from cart 2 / 19 your is empty continue searching add content.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from docx import Document\n",
    "from fpdf import FPDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# Ensure required NLTK resources are available\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Function to scrape data from a webpage\n",
    "def scrape_website(url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        return soup.get_text(separator='\\n')\n",
    "    else:\n",
    "        print(f\"Failed to scrape the website. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Function to clean scraped text\n",
    "def clean_text(text):\n",
    "    # Remove unnecessary information using regular expressions\n",
    "    cleaned_text = re.sub(r'(subscribe|find your story|download media|follow us|contact us|privacy notice|cookie policy|social media|legal|corporate news|cookies|site uses cookies|download all|media cart|press release|related links)', '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Remove extra whitespace and fix inconsistent capitalization\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    \n",
    "    # Fix sentence capitalization\n",
    "    cleaned_text = '. '.join(sentence.capitalize() for sentence in cleaned_text.split('. '))\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Function to clean text for PDF by removing problematic Unicode characters\n",
    "def clean_text_for_pdf(text):\n",
    "    cleaned_text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('ascii')\n",
    "    return cleaned_text\n",
    "\n",
    "# Function to save cleaned text to DOCX with URL\n",
    "def save_to_docx(cleaned_text, url, output_path='Scraped_Content.docx'):\n",
    "    doc = Document()\n",
    "    doc.add_heading('Scraped Content', level=1)\n",
    "\n",
    "    # Add the URL in italic style\n",
    "    para = doc.add_paragraph()\n",
    "    run = para.add_run(f\"URL: {url}\")\n",
    "    run.italic = True\n",
    "\n",
    "    paragraphs = cleaned_text.split(\"\\n\\n\")\n",
    "    for para in paragraphs:\n",
    "        doc.add_paragraph(para)\n",
    "    \n",
    "    doc.save(output_path)\n",
    "    print(f\"Cleaned content saved to {output_path}\")\n",
    "\n",
    "# Function to save cleaned text to PDF with URL\n",
    "def save_to_pdf(cleaned_text, url, output_path='Scraped_Content.pdf'):\n",
    "    pdf = FPDF()\n",
    "    pdf.set_auto_page_break(auto=True, margin=15)\n",
    "    pdf.add_page()\n",
    "    pdf.set_font(\"Arial\", size=12)\n",
    "\n",
    "    # Add URL at the top of the PDF\n",
    "    pdf.multi_cell(0, 10, f\"URL: {url}\\n\")\n",
    "\n",
    "    # Clean the text for PDF to handle encoding issues\n",
    "    cleaned_data_for_pdf = clean_text_for_pdf(cleaned_text)\n",
    "\n",
    "    # Add cleaned text\n",
    "    pdf.multi_cell(0, 10, cleaned_data_for_pdf)\n",
    "    \n",
    "    pdf.output(output_path)\n",
    "    print(f\"Cleaned content saved to {output_path}\")\n",
    "\n",
    "# Function to preprocess text for summarization\n",
    "def preprocess_text(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    processed_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence.lower())\n",
    "        words_filtered = [word for word in words if word.isalnum() and word not in stop_words]\n",
    "        processed_sentences.append(\" \".join(words_filtered))\n",
    "    \n",
    "    return sentences, processed_sentences\n",
    "\n",
    "# Function to perform extractive summarization using TF-IDF\n",
    "def extractive_summary(text, num_sentences=5):\n",
    "    original_sentences, processed_sentences = preprocess_text(text)\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(processed_sentences)\n",
    "\n",
    "    sentence_scores = tfidf_matrix.sum(axis=1)\n",
    "    ranked_sentences = [(score, sent) for score, sent in zip(sentence_scores, original_sentences)]\n",
    "    ranked_sentences.sort(reverse=True, key=lambda x: x[0])\n",
    "\n",
    "    summary = \" \".join([sent for _, sent in ranked_sentences[:num_sentences]])\n",
    "    return summary\n",
    "\n",
    "# Main function to scrape, clean, save, and summarize data\n",
    "def main():\n",
    "    # URL to scrape\n",
    "    url = \"https://news.adidas.com/training/adidas-brings-universal-design-principles-to-kit-for-paris-2024--to-optimise-fit-and-performance-for/s/675a0e1a-ddd7-428f-b15d-1713d59bb352\"\n",
    "\n",
    "    # Step 2: Scrape the website content\n",
    "    scraped_data = scrape_website(url)\n",
    "    \n",
    "    if scraped_data:\n",
    "        # Step 3: Clean the scraped text\n",
    "        cleaned_text = clean_text(scraped_data)\n",
    "        \n",
    "        # Step 4: Save the cleaned content to a new DOCX file with URL\n",
    "        save_to_docx(cleaned_text, url, output_path='Scraped_Content.docx')\n",
    "        \n",
    "        # Step 5: Optionally, save the cleaned content to a PDF file with URL\n",
    "        save_to_pdf(cleaned_text, url, output_path='Scraped_Content.pdf')\n",
    "\n",
    "        # Step 6: Ask the user if they want to generate a summary\n",
    "        summary_choice = input(\"Would you like to generate a summary? (y/n): \")\n",
    "        if summary_choice.lower() == 'y':\n",
    "            summary = extractive_summary(cleaned_text, num_sentences=5)\n",
    "            print(\"\\nSummary:\\n\", summary)\n",
    "        else:\n",
    "            print(\"Summary generation skipped.\")\n",
    "    else:\n",
    "        print(\"Failed to scrape the website. Exiting the process.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03d130ca-5dbe-4bb4-b0fe-bdbb5fab91c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted content saved to Formatted_Scraped_Content.docx\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Would you like to generate a summary? (y/n):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary:\n",
      " African Republic Chad Chile China Colombia Comoros Congo Cook Island Costa Rica Croatia Cuba Cyprus Czech Republic Denmark Djibouti Dominica Dominican Republic Ecuador Egypt El Salvador Equatorial Guinea Eritrea Estonia Ethiopia Fiji Finland France French Polynesia Gabon Gambia Georgia Germany Ghana Gibraltar Greece Greenland Grenada Guadeloupe Guam Guatemala Guinea Guinea-Bissau Guyana Haiti Honduras Hong Kong Hungary Iceland India Indonesia Iran Iraq Ireland Israel Italy Ivory Coast Jamaica Japan Jordan Kazakhstan Kenya Korea, Republic Kosovo Kuwait Kyrgyzstan Laos Latvia Lebanon Lesotho Liberia Libya Liechtenstein Lithuania Lithunia Luxembourg Macau Macedonia Madagascar Madeira Islands Malawi Malaysia Maldives Mali Malta Marshall Islands Martinique Mauritania Mauritius Mexico Micronesia Moldova Monaco Mongolia Montenegro Montserrat Morocco Mozambique Myanmar (Burma) Namibia Nepal Netherlands Netherlands Antilles New Caledonia New Zealand Nicaragua Niger Nigeria North Korea Norway Oman Others Pakistan Palau Palestine Panama Papua New Guinea Paraguay Peru Philippines Poland Portugal Puerto Rico Qatar Republic of Ireland Republic of Kiribati Reunion Islands Romania Russia Rwanda S.Tome and Principe Saba Saint Vincent and the Grenadines San Marino Saudi Arabia Scotland Senegal Serbia Seychelles Sierra Leone Singapore Slovak Republic Slovakia Slovenia Solomon Islands Somalia South Africa South Korea Spain Sri Lanka St Kitts&Nevi St Kitts&Nevis St. Lucia St. Maarten Sudan Suriname Swaziland Sweden Switzerland Syria Taiwan Tajikistan Tanzania Thailand Timor Togo Tonga Trinidad & Tobago Tunisia Turkey Turkmenistan Turks & Caicos Tuvalu Uganda Ukraine United Arab Emirates United Kingdom United States Uruguay Uzbekistan Vanuatu Vatican City Venezuela Vietnam Wales Western Samoa Yemen Zaire Zambia Zimbabwe Message * First Name * Last Name * Email address * WHO ARE YOU? adidas brings universal design principles to kit for Paris 2024, to optimise fit and performance for all news SPORTS PERFORMANCE American Football Baseball Basketball Football Golf Ice Hockey Outdoor Rugby Running Skateboarding Swimming Tennis Training BRANDS Originals Sportswear Partnerships PRODUCT NEWS Footwear Apparel Teams PURPOSE People Planet Innovations FUTURECRAFT 4D Boost Primeknit Back Home Training adidas brings universal design principles to kit for Paris 2024, to optimise fit and performance for all 18-APR-2024 Paris Created in close collaboration with athletes with and without a disability, most of adidas’ on and off field pieces feature key design edits to enable all to participate in comfort adidas launches adaptive training wear pieces for the first time and introduces the brand’s first pattern and finishes crafted specifically for athletes who play sports while seated or in a wheelchair Product featuring universal design – including the adaptive training outfit – are available from 19th April, on www.adidas.com and at key retailers Today, at the reveal of the adidas kits for the Paris 2024 Olympic and Paralympic Games, the sports brand has announced that 81% of pieces of apparel worn on and off the field of play have been created using design principles that ensure they work for athletes with and without a disability. These considerations – applied in varying ways across the collections – include smart placement and/or inclusion of seams, avoiding light colours on the cuffs of performance wear to optimise aesthetics following dirt transfer from wheels, using soft threading where possible, moving team iconography from covered areas – such as the lower back – onto those that are visible while seated, changing trims to be adaptive-first to work around limited movement, removal of any complicated fastenings including small buttons or zips, and offering zipped ankle cuffs for ease of dressing for those with limb difference, and opening a wider range of product lengths for athletes of different heights. Accept & Close CATEGORY OF INTEREST Sports adidas by adidas by Stella McCartney adidas Pharrell Williams Athletics Originals Partnerships Sportswear Y-3 YEEZY Other COUNTRY OF ORIGIN Afghanistan Albania Algeria American Samoa Andorra Angola Anguilla Antigua/Barbuda Argentina Armenia Aruba Australia Austria Azerbaijan Azores Bahamas Bahrain Bangladesh Barbados Belarus Belgium Belize Benin Bermuda Bhutan Bolivia Bosnia-Herzegovina Botswana Brazil British Virgin Islands Brunei Bulgaria Burkino Faso Burundi Cambodia Cameroon Canada Cape Verde Cayman Islands Cen. For guidance on relevant language to tell these stories – please consult the UN’s Disability Inclusive Communications Guidelines [1] World Health Organisation [2] Due to additional partner signings as of July 2024, 81% of base styles have been created using universal design principles ADD ALL TO CART +16 EN adidas brings universal design principles to kit for Paris 2024, to optimise fit and performance for all adidas adidas Group Related Document Disability-Inclusive Communications Guidelines DOWNLOAD FILE ADD FILE TO View 2 / 19 FILES ADD ALL FILES TO CART view back to top DOWNLOAD FILE Remove file from cart 2 / 19 Your is empty Continue Searching Add Content.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from docx import Document\n",
    "from docx.shared import Pt\n",
    "from docx.enum.text import WD_PARAGRAPH_ALIGNMENT\n",
    "from fpdf import FPDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# Ensure required NLTK resources are available\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Function to scrape data from a webpage\n",
    "def scrape_website(url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        return soup.get_text(separator='\\n')\n",
    "    else:\n",
    "        print(f\"Failed to scrape the website. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Function to clean scraped text without affecting punctuation and case\n",
    "def clean_text(text):\n",
    "    # Remove unnecessary information using regular expressions\n",
    "    cleaned_text = re.sub(r'(subscribe|find your story|download media|follow us|contact us|privacy notice|cookie policy|social media|legal|corporate news|cookies|site uses cookies|download all|media cart|press release|related links)', '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "\n",
    "    # Preserve the original case and punctuation by avoiding any further modification\n",
    "    return cleaned_text\n",
    "\n",
    "# Function to save cleaned text to DOCX with URL and formatting\n",
    "def save_to_docx(cleaned_text, url, output_path='Addidas.docx'):\n",
    "    doc = Document()\n",
    "    \n",
    "    # Add a formatted title (Heading 1)\n",
    "    doc.add_heading('Scraped Content from Adidas Website', level=1)\n",
    "\n",
    "    # Add the URL in italic style\n",
    "    para = doc.add_paragraph()\n",
    "    run = para.add_run(f\"URL: {url}\")\n",
    "    run.italic = True\n",
    "    run.font.size = Pt(10)\n",
    "    para.alignment = WD_PARAGRAPH_ALIGNMENT.LEFT\n",
    "\n",
    "    # Add a subtitle\n",
    "    doc.add_heading('Content Summary', level=2)\n",
    "\n",
    "    # Add the cleaned text with custom formatting\n",
    "    paragraphs = cleaned_text.split(\"\\n\\n\")\n",
    "    for para in paragraphs:\n",
    "        p = doc.add_paragraph()\n",
    "        run = p.add_run(para)\n",
    "        run.font.size = Pt(11)\n",
    "        run.font.name = 'Arial'\n",
    "        p.alignment = WD_PARAGRAPH_ALIGNMENT.JUSTIFY\n",
    "\n",
    "    # Set font for the entire document\n",
    "    set_document_font(doc, font_name=\"Arial\", font_size=11)\n",
    "\n",
    "    doc.save(output_path)\n",
    "    print(f\"Formatted content saved to {output_path}\")\n",
    "\n",
    "# Helper function to set the font for the entire document\n",
    "def set_document_font(document, font_name=\"Arial\", font_size=11):\n",
    "    \"\"\"Applies font and size to all paragraphs in a document.\"\"\"\n",
    "    for paragraph in document.paragraphs:\n",
    "        for run in paragraph.runs:\n",
    "            run.font.name = font_name\n",
    "            run.font.size = Pt(font_size)\n",
    "\n",
    "# Function to clean text for PDF by removing problematic Unicode characters\n",
    "def clean_text_for_pdf(text):\n",
    "    cleaned_text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('ascii')\n",
    "    return cleaned_text\n",
    "\n",
    "# Function to preprocess text for summarization\n",
    "def preprocess_text(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    processed_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence.lower())\n",
    "        words_filtered = [word for word in words if word.isalnum() and word not in stop_words]\n",
    "        processed_sentences.append(\" \".join(words_filtered))\n",
    "    \n",
    "    return sentences, processed_sentences\n",
    "\n",
    "# Function to perform extractive summarization using TF-IDF\n",
    "def extractive_summary(text, num_sentences=5):\n",
    "    original_sentences, processed_sentences = preprocess_text(text)\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(processed_sentences)\n",
    "\n",
    "    sentence_scores = tfidf_matrix.sum(axis=1)\n",
    "    ranked_sentences = [(score, sent) for score, sent in zip(sentence_scores, original_sentences)]\n",
    "    ranked_sentences.sort(reverse=True, key=lambda x: x[0])\n",
    "\n",
    "    summary = \" \".join([sent for _, sent in ranked_sentences[:num_sentences]])\n",
    "    return summary\n",
    "\n",
    "# Main function to scrape, clean, save, and summarize data\n",
    "def main():\n",
    "    # URL to scrape\n",
    "    url = \"https://news.adidas.com/training/adidas-brings-universal-design-principles-to-kit-for-paris-2024--to-optimise-fit-and-performance-for/s/675a0e1a-ddd7-428f-b15d-1713d59bb352\"\n",
    "\n",
    "    # Step 2: Scrape the website content\n",
    "    scraped_data = scrape_website(url)\n",
    "    \n",
    "    if scraped_data:\n",
    "        # Step 3: Clean the scraped text\n",
    "        cleaned_text = clean_text(scraped_data)\n",
    "        \n",
    "        # Step 4: Save the cleaned content to a new formatted DOCX file with URL\n",
    "        save_to_docx(cleaned_text, url, output_path='Formatted_Scraped_Content.docx')\n",
    "\n",
    "        # Step 5: Ask the user if they want to generate a summary\n",
    "        summary_choice = input(\"Would you like to generate a summary? (y/n): \")\n",
    "        if summary_choice.lower() == 'y':\n",
    "            summary = extractive_summary(cleaned_text, num_sentences=5)\n",
    "            print(\"\\nSummary:\\n\", summary)\n",
    "        else:\n",
    "            print(\"Summary generation skipped.\")\n",
    "    else:\n",
    "        print(\"Failed to scrape the website. Exiting the process.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8d58e4-f23c-409d-8837-a71ed9981110",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
